# -*- coding: utf-8 -*-
"""
Helpfulness Evaluation Scenarios Collection.

This module contains various scenarios for evaluating the helpfulness of AI responses.
All scenarios in this module are derived from RMBBench and RewardBench 2, which are
comprehensive benchmarks for evaluating reward models in large language models.

The scenarios cover different aspects of helpfulness evaluation including:
- Brainstorming: Creative idea generation
- Chat: Natural conversation maintenance
- Classification: Categorization tasks
- Closed QA: Question answering with specific answers
- Code: Programming assistance
- Focus: Concentration on specific topics
- Generation: Content creation
- Math: Mathematical problem solving
- Open QA: Open-ended question answering
- Precise IF: Conditional precision tasks
- Reasoning: Logical thinking and inference
- Rewrite: Text modification and improvement
- Role Playing: Character-based interactions
- Summarization: Content condensation
- Translation: Language conversion

These scenarios help evaluate how well language models can provide helpful responses
across various domains and task types as standardized in RMBBench and RewardBench 2.
"""
